{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Means and Significance Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conser's Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prose baseline\n",
    "\n",
    "Conser 2020, p. 264: \n",
    "> In order to determine how accentual contours might align by chance,\n",
    "it is first necessary to establish a baseline of chance alignment, using a con-\n",
    "trol group. In the prose of Lysias’ *Against Eratosthenes*, for example, the rate of\n",
    "matched accents between sections is **5.6%**, and the rate of compatible syllables\n",
    "is **73.6%**, providing a minimum baseline for chance alignment.\n",
    "\n",
    "Continuing in a footnote:\n",
    "\n",
    "> Random ‘stanza pairs’ were created by pairing odd and even paragraphs of the first 12 sections and trimming the longer section to have the same number of syllables as the shorter.\n",
    "This resulted in six stanza pairs, each containing an average of 86.5 syllables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimeter baseline\n",
    "\n",
    "Most important:\n",
    "\n",
    "> Prose, however, is a poor choice of comparison for poetic texts, because of the effect of\n",
    "metrical responsion. [...] It is not surprising, then, that the percentage of both matched accents and\n",
    "compatible syllables are higher between sections of iambic trimeter, at **9.7%**\n",
    "and **76.9%** respectively.\n",
    "\n",
    "> Random stanza pairs were created by pairing sequential groups of eight lines, drawn from\n",
    "Antigone 1-96 and 162-321 (Prologue and Episode 1). Resolutions were treated as a single\n",
    "syllable. This resulted in sixteen stanza pairs, each containing 96 syllables.\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "16 antistrophic pairs of 2x8 lines (strikingly, 8 is the mean for Aristophanes' cantica too!)\n",
    "\n",
    "Accentual responsion: **9.7%**\n",
    "Compatibility: **76.9%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix: Constructing the Lyric Tetrameter Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strophe line length should match the mean length of the stanzas in the corpus. Constructing 79 baseline cantica would be overdoing a bit, so 16 seems fine. \n",
    "\n",
    "Most importantly, I'm going to make two things beyond Conser:\n",
    "- triadic and quadratic baselines, and\n",
    "- lyric Frankenstain cantica, using metres that appear in multiple songs, most importantly 4 tr^, cr and ar.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nr of sets of responding lines: 518 lines\n",
      "Mean strophe length: 6.181818181818182 ≈ 6 lines\n"
     ]
    }
   ],
   "source": [
    "from src.stats_comp import compatibility_corpus\n",
    "from statistics import mean\n",
    "\n",
    "all_sets = compatibility_corpus('data/compiled/')\n",
    "\n",
    "flat = []\n",
    "length = 0\n",
    "cantica_lengths = []\n",
    "\n",
    "for play in all_sets:\n",
    "    for canticum in play:\n",
    "        for line_group in canticum:\n",
    "            flat.append(line_group)\n",
    "            length += 1\n",
    "    cantica_lengths.append(len(canticum))\n",
    "mean_cantica_length = mean(cantica_lengths)\n",
    "\n",
    "print(f'Total nr of sets of responding lines: {length} lines')\n",
    "print(f'Mean strophe length: {mean_cantica_length} ≈ {round(mean_cantica_length)} lines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ach: 4x2x8. \n",
    "Excludes the extrametrical lines 43 and 61, and Pseudartabas weirdness, and also lines with anapests, e.g. \n",
    "    6,7 (36, 37 instead)\n",
    "\n",
    "cantica = [\n",
    "    [(1, 8), (9, 17)],\n",
    "    [(18, 26), (27, 35)],\n",
    "    [(72, 80), (81, 89)],\n",
    "    [(108, 116), (117, 125)]\n",
    "]\n",
    "\n",
    "Eq. 4x2x8\n",
    "\n",
    "cantica = [\n",
    "    [(18, 26), (27, 35)],\n",
    "    [(72, 80), (81, 89)],\n",
    "    [(108, 116), (117, 125)],\n",
    "    [(126, 134), (135, 143)]\n",
    "]\n",
    "\n",
    "Nu. \n",
    "Av.\n",
    "\n",
    "Let's find trochaic tetrameter catalectics, and make a pseudo-canticum with them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error reading file 'scan/responsion_ach_scan.xml': failed to load external entity \"scan/responsion_ach_scan.xml\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m xml_file \u001b[38;5;129;01min\u001b[39;00m compiled:\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         tree = \u001b[43metree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxml_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m tree.xpath(\u001b[33m\"\u001b[39m\u001b[33m//l[@metre=\u001b[39m\u001b[33m'\u001b[39m\u001b[33m4 tr^\u001b[39m\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     21\u001b[39m             text = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(l.itertext()) \u001b[38;5;66;03m# good lxml method to know; recursively joins all texts and tails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/etree.pyx:3589\u001b[39m, in \u001b[36mlxml.etree.parse\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:1958\u001b[39m, in \u001b[36mlxml.etree._parseDocument\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:1984\u001b[39m, in \u001b[36mlxml.etree._parseDocumentFromURL\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:1887\u001b[39m, in \u001b[36mlxml.etree._parseDocFromFile\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:1200\u001b[39m, in \u001b[36mlxml.etree._BaseParser._parseDocFromFile\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:633\u001b[39m, in \u001b[36mlxml.etree._ParserContext._handleParseResultDoc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:743\u001b[39m, in \u001b[36mlxml.etree._handleParseResult\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32msrc/lxml/parser.pxi:670\u001b[39m, in \u001b[36mlxml.etree._raiseParseError\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Error reading file 'scan/responsion_ach_scan.xml': failed to load external entity \"scan/responsion_ach_scan.xml\""
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from pathlib import Path\n",
    "\n",
    "compiled = [\n",
    "    Path('scan') / file for file in [\n",
    "        'responsion_ach_scan.xml', \n",
    "        'responsion_av_scan.xml', \n",
    "        'responsion_eq_scan.xml', \n",
    "        'responsion_nu_scan.xml', \n",
    "        'responsion_pax_scan.xml', \n",
    "        'responsion_v_scan.xml'\n",
    "    ]\n",
    "]\n",
    "\n",
    "trochaic_tetrameter_catalectic = []\n",
    "\n",
    "for xml_file in compiled:\n",
    "    try:\n",
    "        tree = etree.parse(xml_file)\n",
    "        for l in tree.xpath(\"//l[@metre='4 tr^']\"):\n",
    "            text = ''.join(l.itertext()) # good lxml method to know; recursively joins all texts and tails\n",
    "            title = tree.xpath(\"//title/text()\")[0]\n",
    "            provenience = title + l.attrib['n']\n",
    "            if text == '' or l.attrib.get('skip', 'False') == 'True':\n",
    "                continue\n",
    "            complete_description = [provenience, text]\n",
    "            trochaic_tetrameter_catalectic.append(complete_description)\n",
    "\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "\n",
    "print(trochaic_tetrameter_catalectic)\n",
    "print(f'Lyric trochaic tetrameter catalectics found: {len(trochaic_tetrameter_catalectic)}')\n",
    "\n",
    "with open('scan/lyricbaseline.txt', 'w', encoding='utf-8') as file:\n",
    "    for line in trochaic_tetrameter_catalectic:\n",
    "        file.write(f'<l n=\"{line[0]}\">{line[1]}</l>\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6*3 Null Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Contour compatibility baselines:\n",
      "--------------------------------\n",
      "Trimeter 2-strophes compatibility baseline: 0.7929447852760736\n",
      "Trimeter 3-strophes compatibility baseline: 0.8103975535168195\n",
      "Trimeter 4-strophes compatibility baseline: 0.7522935779816514\n",
      "Tetrameter 2-strophes compatibility baseline: 0.8217391304347826\n",
      "Tetrameter 3-strophes compatibility baseline: 0.8014814814814815\n",
      "Tetrameter 4-strophes compatibility baseline: 0.7651515151515151\n",
      "--------------------------------\n",
      "Total actual corpus compatibility: 0.8205128205128205\n",
      "--------------------------------\n",
      "\n",
      "--------------------------------\n",
      "Accentual responsion baselines:\n",
      "--------------------------------\n",
      "Trimeter 2-strophes accentual baseline: {'acute': 0.2251655629139073, 'grave': 0.034482758620689655, 'circumflex': 0.12307692307692308, 'acute_circumflex': 0.19444444444444445}\n",
      "  acute: \u001b[32m0.2251655629139073\u001b[0m\n",
      "  grave: \u001b[32m0.034482758620689655\u001b[0m\n",
      "  circumflex: \u001b[32m0.12307692307692308\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.19444444444444445\u001b[0m\n",
      "Trimeter 3-strophes accentual baseline: {'acute': 0.125, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.08256880733944955}\n",
      "  acute: \u001b[32m0.125\u001b[0m\n",
      "  grave: \u001b[32m0.0\u001b[0m\n",
      "  circumflex: \u001b[32m0.0\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.08256880733944955\u001b[0m\n",
      "Trimeter 4-strophes accentual baseline: {'acute': 0.03773584905660377, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.026845637583892617}\n",
      "  acute: \u001b[32m0.03773584905660377\u001b[0m\n",
      "  grave: \u001b[32m0.0\u001b[0m\n",
      "  circumflex: \u001b[32m0.0\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.026845637583892617\u001b[0m\n",
      "Tetrameter 2-strophes accentual baseline: {'acute': 0.2893081761006289, 'grave': 0.10526315789473684, 'circumflex': 0.2318840579710145, 'acute_circumflex': 0.2719298245614035}\n",
      "  acute: \u001b[32m0.2893081761006289\u001b[0m\n",
      "  grave: \u001b[32m0.10526315789473684\u001b[0m\n",
      "  circumflex: \u001b[32m0.2318840579710145\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.2719298245614035\u001b[0m\n",
      "Tetrameter 3-strophes accentual baseline: {'acute': 0.11612903225806452, 'grave': 0.04054054054054054, 'circumflex': 0.04477611940298507, 'acute_circumflex': 0.0945945945945946}\n",
      "  acute: \u001b[32m0.11612903225806452\u001b[0m\n",
      "  grave: \u001b[32m0.04054054054054054\u001b[0m\n",
      "  circumflex: \u001b[32m0.04477611940298507\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.0945945945945946\u001b[0m\n",
      "Tetrameter 4-strophes accentual baseline: {'acute': 0.0, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.0}\n",
      "  acute: \u001b[32m0.0\u001b[0m\n",
      "  grave: \u001b[32m0.0\u001b[0m\n",
      "  circumflex: \u001b[32m0.0\u001b[0m\n",
      "  acute_circumflex: \u001b[32m0.0\u001b[0m\n",
      "\n",
      "--------------------------------\n",
      "Barys responsion baselines:\n",
      "--------------------------------\n",
      "Trimeter 2-strophes barys baseline: {'barys_metric': 0.42038216560509556, 'oxys_metric': 0.03225806451612903, 'barys_oxys_metric': 0.3105022831050228}\n",
      "  barys_metric: \u001b[32m0.42038216560509556\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.03225806451612903\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.3105022831050228\u001b[0m\n",
      "Trimeter 3-strophes barys baseline: {'barys_metric': 0.2727272727272727, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.22018348623853212}\n",
      "  barys_metric: \u001b[32m0.2727272727272727\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.0\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.22018348623853212\u001b[0m\n",
      "Trimeter 4-strophes barys baseline: {'barys_metric': 0.10526315789473684, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.08053691275167785}\n",
      "  barys_metric: \u001b[32m0.10526315789473684\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.0\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.08053691275167785\u001b[0m\n",
      "Tetrameter 2-strophes barys baseline: {'barys_metric': 0.45161290322580644, 'oxys_metric': 0.23809523809523808, 'barys_oxys_metric': 0.41228070175438597}\n",
      "  barys_metric: \u001b[32m0.45161290322580644\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.23809523809523808\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.41228070175438597\u001b[0m\n",
      "Tetrameter 3-strophes barys baseline: {'barys_metric': 0.14835164835164835, 'oxys_metric': 0.15, 'barys_oxys_metric': 0.14864864864864866}\n",
      "  barys_metric: \u001b[32m0.14835164835164835\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.15\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.14864864864864866\u001b[0m\n",
      "Tetrameter 4-strophes barys baseline: {'barys_metric': 0.0446927374301676, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.03686635944700461}\n",
      "  barys_metric: \u001b[32m0.0446927374301676\u001b[0m\n",
      "  oxys_metric: \u001b[32m0.0\u001b[0m\n",
      "  barys_oxys_metric: \u001b[32m0.03686635944700461\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from src.stats import accentual_responsion_metric_canticum, accentual_responsion_metric_play\n",
    "from src.stats_barys import barys_oxys_metric_canticum, barys_oxys_metric_play\n",
    "from src.stats_comp import compatibility_corpus, compatibility_play, compatibility_canticum, compatibility_ratios_to_stats\n",
    "\n",
    "trimeter_path = Path('data/compiled/baseline/responsion_baseline_compiled.xml')\n",
    "trimeterpoly_path = Path('data/compiled/baseline/responsion_baselinepoly_compiled.xml')\n",
    "tetrameter_path = Path('data/compiled/baseline/responsion_lyricbaseline_compiled.xml')\n",
    "\n",
    "if not all(p.exists() for p in [tetrameter_path, trimeter_path, trimeterpoly_path]):\n",
    "    print(f'Some baseline file paths do not exist.')\n",
    "\n",
    "baseline_dict = {}\n",
    "\n",
    "#\n",
    "# Compatibility baselines (these are strictly means)\n",
    "#\n",
    "\n",
    "trimeter_2_strophic = compatibility_play(trimeter_path) # 3 antistrophic cantica of 2x8 lines each\n",
    "trimeter_3_strophes = compatibility_canticum(trimeterpoly_path, 'baselinepoly01')\n",
    "trimeter_4_strophes = compatibility_canticum(trimeterpoly_path, 'baselinepoly02')\n",
    "tetrameter_2_strophes = compatibility_canticum(tetrameter_path, 'lyricbaseline01')\n",
    "tetrameter_3_strophes = compatibility_canticum(tetrameter_path, 'lyricbaseline02')\n",
    "tetrameter_4_strophes = compatibility_canticum(tetrameter_path, 'lyricbaseline03')\n",
    "\n",
    "trimeter_2_strophic_baseline_comp = compatibility_ratios_to_stats([trimeter_2_strophic])\n",
    "trimeter_3_strophes_baseline_comp = compatibility_ratios_to_stats([trimeter_3_strophes])\n",
    "trimeter_4_strophes_baseline_comp = compatibility_ratios_to_stats([trimeter_4_strophes])\n",
    "tetrameter_2_strophes_baseline_comp = compatibility_ratios_to_stats([tetrameter_2_strophes])\n",
    "tetrameter_3_strophes_baseline_comp = compatibility_ratios_to_stats([tetrameter_3_strophes])\n",
    "tetrameter_4_strophes_baseline_comp = compatibility_ratios_to_stats([tetrameter_4_strophes])\n",
    "\n",
    "print('--------------------------------')\n",
    "print('Contour compatibility baselines:')\n",
    "print('--------------------------------')\n",
    "print(f'Trimeter 2-strophes compatibility baseline: {trimeter_2_strophic_baseline_comp}')\n",
    "print(f'Trimeter 3-strophes compatibility baseline: {trimeter_3_strophes_baseline_comp}')\n",
    "print(f'Trimeter 4-strophes compatibility baseline: {trimeter_4_strophes_baseline_comp}')\n",
    "print(f'Tetrameter 2-strophes compatibility baseline: {tetrameter_2_strophes_baseline_comp}')\n",
    "print(f'Tetrameter 3-strophes compatibility baseline: {tetrameter_3_strophes_baseline_comp}')\n",
    "print(f'Tetrameter 4-strophes compatibility baseline: {tetrameter_4_strophes_baseline_comp}')\n",
    "\n",
    "print('--------------------------------')\n",
    "all_sets = compatibility_corpus('data/compiled/') # takes a dir path \n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "print(f'Total actual corpus compatibility: {total_comp}')\n",
    "print('--------------------------------')\n",
    "\n",
    "#\n",
    "# Accentual responsion baselines (these are strictly ratios and not means)\n",
    "#\n",
    "\n",
    "trimeter_2_strophic_baseline_acc = accentual_responsion_metric_play(trimeter_path) # 3 antistrophic cantica of 2x8 lines each\n",
    "trimeter_3_strophes_baseline_acc = accentual_responsion_metric_canticum(trimeterpoly_path, 'baselinepoly01')\n",
    "trimeter_4_strophes_baseline_acc = accentual_responsion_metric_canticum(trimeterpoly_path, 'baselinepoly02')\n",
    "tetrameter_2_strophes_baseline_acc = accentual_responsion_metric_canticum(tetrameter_path, 'lyricbaseline01')\n",
    "tetrameter_3_strophes_baseline_acc = accentual_responsion_metric_canticum(tetrameter_path, 'lyricbaseline02')\n",
    "tetrameter_4_strophes_baseline_acc = accentual_responsion_metric_canticum(tetrameter_path, 'lyricbaseline03')\n",
    "\n",
    "print('\\n--------------------------------')\n",
    "print('Accentual responsion baselines:')\n",
    "print('--------------------------------')\n",
    "print(f'Trimeter 2-strophes accentual baseline: {trimeter_2_strophic_baseline_acc}')\n",
    "for key, value in trimeter_2_strophic_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Trimeter 3-strophes accentual baseline: {trimeter_3_strophes_baseline_acc}')\n",
    "for key, value in trimeter_3_strophes_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Trimeter 4-strophes accentual baseline: {trimeter_4_strophes_baseline_acc}')\n",
    "for key, value in trimeter_4_strophes_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 2-strophes accentual baseline: {tetrameter_2_strophes_baseline_acc}')\n",
    "for key, value in tetrameter_2_strophes_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 3-strophes accentual baseline: {tetrameter_3_strophes_baseline_acc}')\n",
    "for key, value in tetrameter_3_strophes_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 4-strophes accentual baseline: {tetrameter_4_strophes_baseline_acc}')\n",
    "for key, value in tetrameter_4_strophes_baseline_acc.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "\n",
    "\n",
    "#\n",
    "# Barys responsion baselines (also ratios, not means)\n",
    "#\n",
    "\n",
    "trimeter_2_strophic_baseline_barys = barys_oxys_metric_play(\"baseline\", baseline=True) # 3 antistrophic cantica of 2x8 lines each\n",
    "trimeter_3_strophes_baseline_barys = barys_oxys_metric_canticum('baselinepoly01', baseline=True)\n",
    "trimeter_4_strophes_baseline_barys = barys_oxys_metric_canticum('baselinepoly02', baseline=True)\n",
    "tetrameter_2_strophes_baseline_barys = barys_oxys_metric_canticum('lyricbaseline01', baseline=True)\n",
    "tetrameter_3_strophes_baseline_barys = barys_oxys_metric_canticum('lyricbaseline02', baseline=True)\n",
    "tetrameter_4_strophes_baseline_barys = barys_oxys_metric_canticum('lyricbaseline03', baseline=True)\n",
    "\n",
    "print('\\n--------------------------------')\n",
    "print('Barys responsion baselines:')\n",
    "print('--------------------------------')\n",
    "\n",
    "print(f'Trimeter 2-strophes barys baseline: {trimeter_2_strophic_baseline_barys}')\n",
    "for key, value in trimeter_2_strophic_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Trimeter 3-strophes barys baseline: {trimeter_3_strophes_baseline_barys}')\n",
    "for key, value in trimeter_3_strophes_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Trimeter 4-strophes barys baseline: {trimeter_4_strophes_baseline_barys}')\n",
    "for key, value in trimeter_4_strophes_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 2-strophes barys baseline: {tetrameter_2_strophes_baseline_barys}')\n",
    "for key, value in tetrameter_2_strophes_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 3-strophes barys baseline: {tetrameter_3_strophes_baseline_barys}')\n",
    "for key, value in tetrameter_3_strophes_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')\n",
    "print(f'Tetrameter 4-strophes barys baseline: {tetrameter_4_strophes_baseline_barys}')\n",
    "for key, value in tetrameter_4_strophes_baseline_barys.items():\n",
    "    print(f'  {key}: \\033[32m{value}\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making src/utils/baselines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline comp dict:\n",
      "  trimeter_2_strophic: 0.7929447852760736\n",
      "  trimeter_3_strophes: 0.8103975535168195\n",
      "  trimeter_4_strophes: 0.7522935779816514\n",
      "  tetrameter_2_strophes: 0.8217391304347826\n",
      "  tetrameter_3_strophes: 0.8014814814814815\n",
      "  tetrameter_4_strophes: 0.7651515151515151\n",
      "\n",
      "Baseline acc dict:\n",
      "  trimeter_2_strophic: {'acute': 0.2251655629139073, 'grave': 0.034482758620689655, 'circumflex': 0.12307692307692308, 'acute_circumflex': 0.19444444444444445}\n",
      "  trimeter_3_strophes: {'acute': 0.125, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.08256880733944955}\n",
      "  trimeter_4_strophes: {'acute': 0.03773584905660377, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.026845637583892617}\n",
      "  tetrameter_2_strophes: {'acute': 0.2893081761006289, 'grave': 0.10526315789473684, 'circumflex': 0.2318840579710145, 'acute_circumflex': 0.2719298245614035}\n",
      "  tetrameter_3_strophes: {'acute': 0.11612903225806452, 'grave': 0.04054054054054054, 'circumflex': 0.04477611940298507, 'acute_circumflex': 0.0945945945945946}\n",
      "  tetrameter_4_strophes: {'acute': 0.0, 'grave': 0.0, 'circumflex': 0.0, 'acute_circumflex': 0.0}\n",
      "\n",
      "Baseline barys dict:\n",
      "  trimeter_2_strophic: {'barys_metric': 0.42038216560509556, 'oxys_metric': 0.03225806451612903, 'barys_oxys_metric': 0.3105022831050228}\n",
      "  trimeter_3_strophes: {'barys_metric': 0.2727272727272727, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.22018348623853212}\n",
      "  trimeter_4_strophes: {'barys_metric': 0.10526315789473684, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.08053691275167785}\n",
      "  tetrameter_2_strophes: {'barys_metric': 0.45161290322580644, 'oxys_metric': 0.23809523809523808, 'barys_oxys_metric': 0.41228070175438597}\n",
      "  tetrameter_3_strophes: {'barys_metric': 0.14835164835164835, 'oxys_metric': 0.15, 'barys_oxys_metric': 0.14864864864864866}\n",
      "  tetrameter_4_strophes: {'barys_metric': 0.0446927374301676, 'oxys_metric': 0.0, 'barys_oxys_metric': 0.03686635944700461}\n"
     ]
    }
   ],
   "source": [
    "baseline_dict = {\n",
    "    'comp': {\n",
    "        'trimeter_2_strophic': trimeter_2_strophic_baseline_comp,\n",
    "        'trimeter_3_strophes': trimeter_3_strophes_baseline_comp,\n",
    "        'trimeter_4_strophes': trimeter_4_strophes_baseline_comp,\n",
    "        'tetrameter_2_strophes': tetrameter_2_strophes_baseline_comp,\n",
    "        'tetrameter_3_strophes': tetrameter_3_strophes_baseline_comp,\n",
    "        'tetrameter_4_strophes': tetrameter_4_strophes_baseline_comp\n",
    "    },\n",
    "    'acc': {\n",
    "        'trimeter_2_strophic': trimeter_2_strophic_baseline_acc,\n",
    "        'trimeter_3_strophes': trimeter_3_strophes_baseline_acc,\n",
    "        'trimeter_4_strophes': trimeter_4_strophes_baseline_acc,\n",
    "        'tetrameter_2_strophes': tetrameter_2_strophes_baseline_acc,\n",
    "        'tetrameter_3_strophes': tetrameter_3_strophes_baseline_acc,\n",
    "        'tetrameter_4_strophes': tetrameter_4_strophes_baseline_acc\n",
    "    },\n",
    "    'barys': {\n",
    "        'trimeter_2_strophic': trimeter_2_strophic_baseline_barys,\n",
    "        'trimeter_3_strophes': trimeter_3_strophes_baseline_barys,\n",
    "        'trimeter_4_strophes': trimeter_4_strophes_baseline_barys,\n",
    "        'tetrameter_2_strophes': tetrameter_2_strophes_baseline_barys,\n",
    "        'tetrameter_3_strophes': tetrameter_3_strophes_baseline_barys,\n",
    "        'tetrameter_4_strophes': tetrameter_4_strophes_baseline_barys\n",
    "    }\n",
    "}\n",
    "\n",
    "for key, value in baseline_dict.items():\n",
    "    print(f'\\nBaseline {key} dict:')\n",
    "    for subkey, subvalue in value.items():\n",
    "        print(f'  {subkey}: {subvalue}')\n",
    "\n",
    "\n",
    "# Pretty-print write the baseline dict to src/utils/baselines.py\n",
    "import pprint\n",
    "\n",
    "with open('src/utils/baselines.py', 'w', encoding='utf-8') as f:\n",
    "    f.write('# This baseline dictionary is generated by a cell in the nb_significance.ipynb notebook.\\n')\n",
    "    f.write('baseline_dict = ')\n",
    "    f.write(pprint.pformat(baseline_dict, width=100))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage of the baseline utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting acute + circumflex responsion baseline for the tristrophic trimeter:\n",
      "0.08256880733944955\n",
      "0.08256880733944955\n"
     ]
    }
   ],
   "source": [
    "from src.utils.utils import baseline\n",
    "\n",
    "# Example usage of the baseline utility function\n",
    "\n",
    "print(\"\\nGetting acute + circumflex responsion baseline for the tristrophic trimeter:\")\n",
    "\n",
    "# 1\n",
    "print(baseline_dict[\"acc\"][\"trimeter_3_strophes\"][\"acute_circumflex\"])\n",
    "\n",
    "# 2\n",
    "print(baseline(\"acc\", \"trimeter_3_strophes\", \"acute_circumflex\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test for the comp mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Antistrophic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean (observed, antistrophic): 0.8221052631578948\n",
      "Number of variables: 6175\n",
      "Distribution variables (Antistrophic):\n",
      "\t1.0: 3978\n",
      "\t0.5: 2197\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info (Antistrophic)\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/', 'antistrophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (observed, antistrophic): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables (Antistrophic):')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic trimeter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean (null, antistrophic trimeter): 0.7929447852760736\n",
      "Number of variables: 326\n",
      "Distribution variables:\n",
      "\t0.5: 135\n",
      "\t1.0: 191\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info (Antistrophic)\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/baseline_trimeter', 'antistrophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (null, antistrophic trimeter): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic mean chi-square test against antistrophic trimeter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 86.5674\n",
      "Degrees of freedom: 1\n",
      "P-value: 1.351e-20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Observed counts\n",
    "obs_counts = np.array([2197, 3978])  # [0.5, 1.0]\n",
    "obs_total = obs_counts.sum()\n",
    "\n",
    "# Null counts → proportions\n",
    "null_counts = np.array([135, 191])\n",
    "null_total = null_counts.sum()\n",
    "null_probs = null_counts / null_total\n",
    "\n",
    "# Expected counts under H₀\n",
    "expected_counts = null_probs * obs_total\n",
    "\n",
    "# Chi-square test\n",
    "chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "# Output\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "print(f\"P-value: {p_value:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic tetrameter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean (null, antistrophic tetrameter): 0.8217391304347826\n",
      "Number of variables: 345\n",
      "Distribution variables:\n",
      "\t1.0: 222\n",
      "\t0.5: 123\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info (Antistrophic)\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/baseline_tetrameter', 'antistrophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (null, antistrophic tetrameter): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic tetrameter null SHUFFLED, 1 to 10 versions together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean (null, antistrophic tetrameter): 0.8043478260869565\n",
      "Number of variables: 345\n",
      "Distribution variables:\n",
      "\t1.0: 210\n",
      "\t0.5: 135\n",
      "---------------------------------\n",
      "2 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8057971014492754\n",
      "Number of variables: 690\n",
      "Distribution variables:\n",
      "\t1.0: 422\n",
      "\t0.5: 268\n",
      "---------------------------------\n",
      "3 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8091787439613527\n",
      "Number of variables: 1035\n",
      "Distribution variables:\n",
      "\t1.0: 640\n",
      "\t0.5: 395\n",
      "---------------------------------\n",
      "4 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8152173913043478\n",
      "Number of variables: 1380\n",
      "Distribution variables:\n",
      "\t0.5: 510\n",
      "\t1.0: 870\n",
      "---------------------------------\n",
      "5 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8147826086956522\n",
      "Number of variables: 1725\n",
      "Distribution variables:\n",
      "\t1.0: 1086\n",
      "\t0.5: 639\n",
      "---------------------------------\n",
      "6 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8178743961352657\n",
      "Number of variables: 2070\n",
      "Distribution variables:\n",
      "\t1.0: 1316\n",
      "\t0.5: 754\n",
      "---------------------------------\n",
      "7 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8146997929606625\n",
      "Number of variables: 2415\n",
      "Distribution variables:\n",
      "\t1.0: 1520\n",
      "\t0.5: 895\n",
      "---------------------------------\n",
      "8 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8141304347826087\n",
      "Number of variables: 2760\n",
      "Distribution variables:\n",
      "\t1.0: 1734\n",
      "\t0.5: 1026\n",
      "---------------------------------\n",
      "9 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.813365539452496\n",
      "Number of variables: 3105\n",
      "Distribution variables:\n",
      "\t1.0: 1946\n",
      "\t0.5: 1159\n",
      "---------------------------------\n",
      "10 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8139130434782609\n",
      "Number of variables: 3450\n",
      "Distribution variables:\n",
      "\t0.5: 1284\n",
      "\t1.0: 2166\n",
      "---------------------------------\n",
      "11 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8122529644268774\n",
      "Number of variables: 3795\n",
      "Distribution variables:\n",
      "\t0.5: 1425\n",
      "\t1.0: 2370\n",
      "---------------------------------\n",
      "12 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8106280193236715\n",
      "Number of variables: 4140\n",
      "Distribution variables:\n",
      "\t1.0: 2572\n",
      "\t0.5: 1568\n",
      "---------------------------------\n",
      "13 shuffled versions\n",
      "Compatibility mean (null, antistrophic tetrameter): 0.8103678929765886\n",
      "Number of variables: 4485\n",
      "Distribution variables:\n",
      "\t1.0: 2784\n",
      "\t0.5: 1701\n",
      "---------------------------------\n",
      "defaultdict(<class 'dict'>, {2: Counter({1.0: 422, 0.5: 268}), 3: Counter({1.0: 640, 0.5: 395}), 4: Counter({1.0: 870, 0.5: 510}), 5: Counter({1.0: 1086, 0.5: 639}), 6: Counter({1.0: 1316, 0.5: 754}), 7: Counter({1.0: 1520, 0.5: 895}), 8: Counter({1.0: 1734, 0.5: 1026}), 9: Counter({1.0: 1946, 0.5: 1159}), 10: Counter({1.0: 2166, 0.5: 1284}), 11: Counter({1.0: 2370, 0.5: 1425}), 12: Counter({1.0: 2572, 0.5: 1568}), 13: Counter({1.0: 2784, 0.5: 1701})})\n",
      "{2: {1.0: 422, 0.5: 268}, 3: {1.0: 640, 0.5: 395}, 4: {0.5: 510, 1.0: 870}, 5: {1.0: 1086, 0.5: 639}, 6: {1.0: 1316, 0.5: 754}, 7: {1.0: 1520, 0.5: 895}, 8: {1.0: 1734, 0.5: 1026}, 9: {1.0: 1946, 0.5: 1159}, 10: {0.5: 1284, 1.0: 2166}, 11: {0.5: 1425, 1.0: 2370}, 12: {1.0: 2572, 0.5: 1568}, 13: {1.0: 2784, 0.5: 1701}}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info (Antistrophic)\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/baseline_tetrameter_shuffled', 'antistrophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (null, antistrophic tetrameter): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')\n",
    "print('---------------------------------')\n",
    "\n",
    "# Making the other 9 shuffled null distributions\n",
    "\n",
    "# import default dict\n",
    "from collections import defaultdict\n",
    "shuffled_distributions = defaultdict(dict)\n",
    "\n",
    "# Make dict with the dist pair for each i\n",
    "for i in range(2, 21):\n",
    "    all_sets = compatibility_strophicity(f'data/compiled/baseline_tetrameter_shuffled{i}', 'antistrophic')\n",
    "    total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "    print(f'{i} shuffled versions')\n",
    "    print(f'Compatibility mean (null, antistrophic tetrameter): {total_comp}')\n",
    "\n",
    "    number_of_variables = 0\n",
    "\n",
    "    values = []\n",
    "    for element in all_sets:\n",
    "        for subelement in element:\n",
    "            for subsubelement in subelement:\n",
    "                for value in subsubelement:\n",
    "                    number_of_variables += 1\n",
    "                    values.append(value)\n",
    "                    \n",
    "    print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "    count_dict = Counter(values)\n",
    "    print(f'Distribution variables:')\n",
    "    for key, value in count_dict.items():\n",
    "        print(f'\\t{key}: {value}')\n",
    "    print('---------------------------------')\n",
    "    shuffled_distributions[i] = count_dict\n",
    "\n",
    "print(shuffled_distributions)\n",
    "\n",
    "# Make the dict in to a normal dict again\n",
    "shuffled_distributions = {k: dict(v) for k, v in shuffled_distributions.items()}\n",
    "print(shuffled_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic mean chi-square test against antistrophic tetrameter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 0.0144\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.904 (≈ 10^-1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "\n",
    "# Observed counts\n",
    "obs_counts = np.array([2197, 3978])  # [0.5, 1.0]\n",
    "obs_total = obs_counts.sum()\n",
    "\n",
    "# Updated null counts from antistrophic tetrameter\n",
    "null_counts = np.array([123, 222])  # [0.5, 1.0]\n",
    "null_total = null_counts.sum()\n",
    "null_probs = null_counts / null_total\n",
    "\n",
    "# Expected counts under null\n",
    "expected_counts = null_probs * obs_total\n",
    "\n",
    "# Chi-square test\n",
    "chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "# Output\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "if p_value > 0:\n",
    "    print(f\"P-value: {p_value:.3f} (≈ 10^{math.floor(math.log10(p_value))})\")\n",
    "else:\n",
    "    print(\"P-value is effectively zero (underflow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antistrophic mean chi-square test against SHUFFLED antistrophic tetrameter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square statistic: 1.9093\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.167 (≈ 10^-1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "\n",
    "# Observed counts\n",
    "obs_counts = np.array([2197, 3978])  # [0.5, 1.0]\n",
    "obs_total = obs_counts.sum()\n",
    "\n",
    "# Updated null counts from antistrophic tetrameter\n",
    "null_counts = np.array([754, 1316])  # [0.5, 1.0]\n",
    "null_total = null_counts.sum()\n",
    "null_probs = null_counts / null_total\n",
    "\n",
    "# Expected counts under null\n",
    "expected_counts = null_probs * obs_total\n",
    "\n",
    "# Chi-square test\n",
    "chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "# Output\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "if p_value > 0:\n",
    "    print(f\"P-value: {p_value:.3f} (≈ 10^{math.floor(math.log10(p_value))})\")\n",
    "else:\n",
    "    print(\"P-value is effectively zero (underflow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-square test using all ever-growing concatenations of 10 shuffled distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing distribution 2:\n",
      "Chi-square statistic: 27.6540\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0000 (≈ 10^-7)\n",
      "\n",
      "Testing distribution 3:\n",
      "Chi-square statistic: 17.4890\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0000 (≈ 10^-5)\n",
      "\n",
      "Testing distribution 4:\n",
      "Chi-square statistic: 5.0296\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0249 (≈ 10^-2)\n",
      "\n",
      "Testing distribution 5:\n",
      "Chi-square statistic: 5.6791\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0172 (≈ 10^-2)\n",
      "\n",
      "Testing distribution 6:\n",
      "Chi-square statistic: 1.9093\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.1670 (≈ 10^-1)\n",
      "\n",
      "Testing distribution 7:\n",
      "Chi-square statistic: 5.8072\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0160 (≈ 10^-2)\n",
      "\n",
      "Testing distribution 8:\n",
      "Chi-square statistic: 6.7261\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0095 (≈ 10^-3)\n",
      "\n",
      "Testing distribution 9:\n",
      "Chi-square statistic: 8.0647\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0045 (≈ 10^-3)\n",
      "\n",
      "Testing distribution 10:\n",
      "Chi-square statistic: 7.0944\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0077 (≈ 10^-3)\n",
      "\n",
      "Testing distribution 11:\n",
      "Chi-square statistic: 10.2243\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0014 (≈ 10^-3)\n",
      "\n",
      "Testing distribution 12:\n",
      "Chi-square statistic: 13.8279\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0002 (≈ 10^-4)\n",
      "\n",
      "Testing distribution 13:\n",
      "Chi-square statistic: 14.4541\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.0001 (≈ 10^-4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "\n",
    "for i in range(2, 14):\n",
    "    print(f'\\nTesting distribution {i}:')\n",
    "    distribution = shuffled_distributions[i]\n",
    "\n",
    "    # Observed counts\n",
    "    obs_counts = np.array([2197, 3978])  # [0.5, 1.0]\n",
    "    obs_total = obs_counts.sum()\n",
    "\n",
    "    # Updated null counts from antistrophic tetrameter\n",
    "    null_counts = np.array([distribution[0.5], distribution[1]])  # [0.5, 1.0]\n",
    "    null_total = null_counts.sum()\n",
    "    null_probs = null_counts / null_total\n",
    "\n",
    "    # Expected counts under null\n",
    "    expected_counts = null_probs * obs_total\n",
    "\n",
    "    # Chi-square test\n",
    "    chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "    # Output\n",
    "    print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "    if p_value > 0:\n",
    "        print(f\"P-value: {p_value:.4f} (≈ 10^{math.floor(math.log10(p_value))})\")\n",
    "    else:\n",
    "        print(\"P-value is effectively zero (underflow)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3 and 4-strophic distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Polystrophic observed distribution\n",
    "2. Polystrophic trimeter null\n",
    "3. Chi-square test against trimeter null\n",
    "4. Polystrophic tetrameter null\n",
    "5. Chi-square test against tetrameter null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Observed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-strophic observed distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean (observed): 0.8172043010752688\n",
      "Number of variables: 155\n",
      "Distribution variables (Polystrophic):\n",
      "\t0.6666666666666666: 85\n",
      "\t1.0: 70\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/', 'three-strophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (observed): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables (Polystrophic):')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-strophic null trimeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean: 0.8103975535168195\n",
      "Number of variables: 109\n",
      "Distribution variables:\n",
      "\t0.6666666666666666: 62\n",
      "\t1.0: 47\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/baseline_trimeter', 'three-strophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean: {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chi-square test comparing observed 3-strophic distribution to 3-strophic trimeter null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test comparing observed polystrophic distribution to trimeter null:\n",
      "Observed total: 155\n",
      "Expected counts under H₀: [88.17 66.83]\n",
      "Chi-square statistic: 0.2635\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.6077 (≈ 10^-1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "\n",
    "# Observed counts\n",
    "# Order: [0.666..., 1.0]\n",
    "obs_counts = np.array([85, 70])\n",
    "obs_total = obs_counts.sum()\n",
    "\n",
    "# Null counts (for expected proportions)\n",
    "null_counts = np.array([62, 47])\n",
    "null_total = null_counts.sum()\n",
    "null_probs = null_counts / null_total\n",
    "\n",
    "# Expected counts under H₀ (scaled to obs_total)\n",
    "expected_counts = null_probs * obs_total\n",
    "\n",
    "# Chi-square test\n",
    "chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "# Output\n",
    "print(\"Chi-square test comparing observed 3-strophic distribution to 3-strophic trimeter null:\")\n",
    "print(f\"Observed total: {obs_total}\")\n",
    "print(f\"Expected counts under H₀: {expected_counts.round(2)}\")\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "if p_value > 0:\n",
    "    print(f\"P-value: {p_value:.4f} (≈ 10^{math.floor(math.log10(p_value))})\")\n",
    "else:\n",
    "    print(\"P-value: < 1e-{0}\".format(abs(int(np.floor(np.log10(np.finfo(float).eps))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-strophic tetrameter null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compatibility mean: 0.8014814814814815\n",
      "Number of variables: 225\n",
      "Distribution variables:\n",
      "\t0.6666666666666666: 134\n",
      "\t1.0: 91\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_strophicity, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info\n",
    "\n",
    "all_sets = compatibility_strophicity('data/compiled/baseline_tetrameter', 'three-strophic')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean: {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "                \n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test comparing observed 3-strophic distribution to 3-strophic TETRAMETER null:\n",
      "Observed total: 155\n",
      "Expected counts under H₀: [92.31 62.69]\n",
      "Chi-square statistic: 1.4317\n",
      "Degrees of freedom: 1\n",
      "P-value: 0.2315 (≈ 10^-1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import chisquare\n",
    "import math\n",
    "\n",
    "# Observed counts\n",
    "# Order: [0.666..., 1.0]\n",
    "obs_counts = np.array([85, 70])\n",
    "obs_total = obs_counts.sum()\n",
    "\n",
    "# Null counts (for expected proportions)\n",
    "null_counts = np.array([134, 91])\n",
    "null_total = null_counts.sum()\n",
    "null_probs = null_counts / null_total\n",
    "\n",
    "# Expected counts under H₀ (scaled to obs_total)\n",
    "expected_counts = null_probs * obs_total\n",
    "\n",
    "# Chi-square test\n",
    "chi2_stat, p_value = chisquare(f_obs=obs_counts, f_exp=expected_counts)\n",
    "\n",
    "# Output\n",
    "print(\"Chi-square test comparing observed 3-strophic distribution to 3-strophic TETRAMETER null:\")\n",
    "print(f\"Observed total: {obs_total}\")\n",
    "print(f\"Expected counts under H₀: {expected_counts.round(2)}\")\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"Degrees of freedom: {len(obs_counts) - 1}\")\n",
    "if p_value > 0:\n",
    "    print(f\"P-value: {p_value:.4f} (≈ 10^{math.floor(math.log10(p_value))})\")\n",
    "else:\n",
    "    print(\"P-value: < 1e-{0}\".format(abs(int(np.floor(np.log10(np.finfo(float).eps))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Test for the Comp Mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re computing a mean $\\bar{x}$ over $n$ random variables (representing syllables or accents), which\n",
    "- are bounded in [0, 1],\n",
    "- take values in a discrete set, \n",
    "  - for full corpus\n",
    "$\\left\\{\\frac{1}{2}, \\frac{2}{3}, \\frac{3}{4}, 1 \\right\\}$ (97.0% being binary 0.5 or 1)\n",
    "  - or just binary for any metric on the antistrophic sub-corpus,\n",
    "\n",
    "and of which the true underlying distribution is:\n",
    "- known only empirically (no analytical variance available!),\n",
    "- not symmetric, not uniform, and heavily bimodal (binary),\n",
    "\n",
    "and then comparing with a known baseline mean under the null, $\\mu_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this well we need information about the distribution $\\bar{x}$ comes from. It turns out the two lowest values $\\frac{1}{4}$ and $\\frac{1}{3}$ actually never occur, so there are only $\\left\\{\\frac{1}{4}, \\frac{1}{3}, \\frac{1}{2}, \\frac{2}{3}\\right\\}$. (This means that among 3 or 4 responding strophes there are no syllables where each all are incompatible; this makes sense, since there are only two directions for the contour to go, since the flat is compatible with either by definition!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_corpus, compatibility_ratios_to_stats\n",
    "\n",
    "# Distribution info (Full-corpus)\n",
    "\n",
    "all_sets = compatibility_corpus('data/compiled/')\n",
    "total_comp = compatibility_ratios_to_stats(all_sets)\n",
    "\n",
    "print(f'Compatibility mean (observed): {total_comp}')\n",
    "\n",
    "number_of_variables = 0\n",
    "\n",
    "values = []\n",
    "for element in all_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables += 1\n",
    "                values.append(value)\n",
    "\n",
    "print(f'Number of variables: {number_of_variables}')\n",
    "\n",
    "count_dict = Counter(values)\n",
    "print(f'Distribution variables:')\n",
    "for key, value in count_dict.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trimeter Null Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Compatibility mean under trimeter null (m0): 0.7966510294500913\n",
      "Number of variables (null): 1279\n",
      "Null distribution variables:\n",
      "\t0.6666666666666666: 196\n",
      "\t1.0: 632\n",
      "\t0.5: 328\n",
      "\t0.75: 123\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_corpus, compatibility_ratios_to_stats\n",
    "\n",
    "# Trimeter Null distribution info\n",
    "\n",
    "null_sets = compatibility_corpus('data/compiled/baseline')\n",
    "null_comp = compatibility_ratios_to_stats(null_sets)\n",
    "\n",
    "print('--------------------------------')\n",
    "print(f'Compatibility mean under trimeter null (m0): {null_comp}')\n",
    "\n",
    "number_of_variables_null = 0\n",
    "\n",
    "values_null = []\n",
    "for element in null_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables_null += 1\n",
    "                values_null.append(value)\n",
    "\n",
    "print(f'Number of variables (null): {number_of_variables_null}')\n",
    "\n",
    "count_dict_null = Counter(values_null)\n",
    "print(f'Null distribution variables:')\n",
    "for key, value in count_dict_null.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tetrameter Null Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Compatibility mean under tetrameter null (m0): 0.8028344671201814\n",
      "Number of variables (null): 735\n",
      "Null distribution variables:\n",
      "\t0.6666666666666666: 134\n",
      "\t1.0: 364\n",
      "\t0.5: 164\n",
      "\t0.75: 73\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from src.stats_comp import compatibility_corpus, compatibility_ratios_to_stats\n",
    "\n",
    "# Tetrameter Null distribution info\n",
    "\n",
    "null_sets = compatibility_corpus('data/compiled/baseline_tetrameter')\n",
    "null_comp = compatibility_ratios_to_stats(null_sets)\n",
    "\n",
    "print('--------------------------------')\n",
    "print(f'Compatibility mean under tetrameter null (m0): {null_comp}')\n",
    "\n",
    "number_of_variables_null = 0\n",
    "\n",
    "values_null = []\n",
    "for element in null_sets:\n",
    "    for subelement in element:\n",
    "        for subsubelement in subelement:\n",
    "            for value in subsubelement:\n",
    "                number_of_variables_null += 1\n",
    "                values_null.append(value)\n",
    "\n",
    "print(f'Number of variables (null): {number_of_variables_null}')\n",
    "\n",
    "count_dict_null = Counter(values_null)\n",
    "print(f'Null distribution variables:')\n",
    "for key, value in count_dict_null.items():\n",
    "    print(f'\\t{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acute-circumflex and barys ratio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binomial significance test for the binary acute-circumflex and barys-only ratio metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
