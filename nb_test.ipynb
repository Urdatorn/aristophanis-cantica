{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fddabcd6",
   "metadata": {},
   "source": [
    "# Manually Checked Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c28bf79",
   "metadata": {},
   "source": [
    "Naturally we need guaranteed true accent and barys responsion stats for at least one antistrophic and one polystrophic song to know that the stats code works.\n",
    "\n",
    "I chose ach01 and ach05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991d890",
   "metadata": {},
   "source": [
    "## Accentual Responsion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c627f2",
   "metadata": {},
   "source": [
    "### Antistrophic: ach01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888e40a",
   "metadata": {},
   "source": [
    "- ach01: \n",
    "  - Accentual responsion (confirms visualization.py): \n",
    "    - Acute:      \n",
    "      - 0 + 3 + 0 + 2 + 0 + 1 + 0 + 1 + 2 = 9\n",
    "    - Grave:      \n",
    "      - 1 + 0 + 0 + 1 + 0 + 0 + 0 + 0 + 0 = 2\n",
    "    - Circumflex: \n",
    "      - 1 + 0 + 0 + 0 + 0 + 1 + 1 + 0 + 1 = 4\n",
    "\n",
    "My manual check confirms the accent map of `accentually_responding_syllables_of_strophes_polystrophic`.\n",
    "\n",
    "Using `count_all_accents_canticum(tree, canticum)` we can get the total number of acutes in both strophes as 62, which I have manually confirmed by reckoning in the file `sanity_check/ach01.tsv`. Before we compute the fraction we have two count every responding syllable twice, because it actually involves two accents. Finally, this means we have a confirmed statistic of **9*2/62 ≈ 0.29**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a64d49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accent_maps = [\n",
    "    [\n",
    "        {('205', 7): 'πάν', ('220', 7): 'τεί'}, \n",
    "        {('205', 10): 'πό', ('220', 10): 'κέ'}, \n",
    "        {('205', 13): 'ἄξ', ('220', 13): 'ρύ'}, \n",
    "        {('207', 4): \"δ' ὅ\", ('222', 4): 'γέ'}, \n",
    "        {('207', 14): 'φέ', ('222', 14): 'νέ'}, \n",
    "        {('210-211', 3): 'ς. Οἴ', ('225', 3): 'οί'}, \n",
    "        {('213-214', 5): 'τί', ('227', 5): 'ρί'}, \n",
    "        {('215-217', 3): 'λού', ('230-233', 3): 'νή'}, \n",
    "        {('215-217', 23): 'τό', ('230-233', 23): 'ρό'}\n",
    "        ], \n",
    "    [\n",
    "        {('204', 10): 'τὸ', ('219', 10): 'μὸ'}, \n",
    "        {('207', 11): 'τὰς', ('222', 11): 'γὼ'}\n",
    "        ],\n",
    "    [\n",
    "        {('204', 1): 'Τῇ', ('219', 1): 'Νῦν'}, \n",
    "        {('210-211', 7): 'τῶν', ('225', 7): 'θροῖ'}, \n",
    "        {('211-212', 4): 'μῆς', ('226', 4): 'μοῦ'}, \n",
    "        {('215-217', 10): 'ὧ', ('230-233', 10): 'τοῖ'}\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc58a25",
   "metadata": {},
   "source": [
    "The easiest way to manually count is to print the sylls from all responding lines aligned in tabs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2beadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as ET\n",
    "\n",
    "def extract_responsion_syllables_vertical(filename: str, responsion_value: str) -> str:\n",
    "    tree = ET.parse(filename)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Collect all <strophe> and <antistrophe> with matching responsion\n",
    "    matched_blocks = []\n",
    "    for tag in ('strophe', 'antistrophe'):\n",
    "        matched_blocks.extend(root.xpath(f\".//{tag}[@responsion='{responsion_value}']\"))\n",
    "\n",
    "    # Extract <l> elements from each matched block\n",
    "    all_l_lists = [block.findall(\"l\") for block in matched_blocks]\n",
    "    max_lines = max(len(l_list) for l_list in all_l_lists)\n",
    "\n",
    "    output_lines = []\n",
    "\n",
    "    for line_index in range(max_lines):\n",
    "        for l_list in all_l_lists:\n",
    "            if line_index < len(l_list):\n",
    "                l_elem = l_list[line_index]\n",
    "\n",
    "                # Get all <syll> elements (including those in <conjecture>)\n",
    "                syll_elems = l_elem.xpath(\".//syll\")\n",
    "\n",
    "                merged_sylls = []\n",
    "                i = 0\n",
    "                while i < len(syll_elems):\n",
    "                    syll = syll_elems[i]\n",
    "                    text = (syll.text or \"\").strip()\n",
    "                    if i + 1 < len(syll_elems):\n",
    "                        next_syll = syll_elems[i + 1]\n",
    "                        if (syll.get(\"resolution\") == \"True\" and\n",
    "                            next_syll.get(\"resolution\") == \"True\"):\n",
    "                            merged = text + (next_syll.text or \"\").strip()\n",
    "                            merged_sylls.append(merged)\n",
    "                            i += 2\n",
    "                            continue\n",
    "                    merged_sylls.append(text)\n",
    "                    i += 1\n",
    "\n",
    "                output_lines.append(\"\\t\".join(merged_sylls))\n",
    "            else:\n",
    "                output_lines.append(\"\")  # No line at this index in this block\n",
    "        output_lines.append(\"\")  # Blank line between line groups\n",
    "\n",
    "    return \"\\n\".join(output_lines)\n",
    "\n",
    "canticum = \"ach05\"\n",
    "infix = canticum[:-2]\n",
    "\n",
    "input_file = f\"data/compiled/responsion_{infix}_compiled.xml\"\n",
    "\n",
    "output_lines = extract_responsion_syllables_vertical(input_file, canticum)\n",
    "\n",
    "print(output_lines)\n",
    "with open(f\"sanity_check/{canticum}.tsv\", \"w\") as f:\n",
    "    f.write(output_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15025dd",
   "metadata": {},
   "source": [
    "### Polystrophic: ach05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1ec2e",
   "metadata": {},
   "source": [
    "`accentually_responding_syllables_of_strophes_polystrophic` yields an empty accent_map for the quartet `ach05`. We can convince ourselves that this is not simply a lack of support for polystrophy by creating a version with an artificial responsion in the first syllable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49383da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from src.stats import accentually_responding_syllables_of_strophes_polystrophic\n",
    "\n",
    "canticum = \"ach05\"\n",
    "infix = canticum[:-2]\n",
    "xml_file = f\"data/compiled/responsion_{infix}_compiled.xml\"\n",
    "xml_file = f\"data/compiled/test/responsion_mockup_compiled.xml\"\n",
    "\n",
    "tree = etree.parse(xml_file)\n",
    "strophes = tree.xpath(f'//*[self::strophe or self::antistrophe][@responsion=\"{canticum}\"]')\n",
    "accent_maps = accentually_responding_syllables_of_strophes_polystrophic(*strophes)\n",
    "\n",
    "print(\"accent_maps: \", accent_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91538a92",
   "metadata": {},
   "source": [
    "Let's try a few other polystrophic songs: eq07 (4), pax01 (3), lys08 (4), ra04 (3), ra08 (4):\n",
    "\n",
    "ach05: {'acute': 0, 'grave': 0, 'circumflex': 0}\n",
    "eq07: {'acute': 0, 'grave': 0, 'circumflex': 0}\n",
    "pax01: {'acute': 6, 'grave': 3, 'circumflex': 0}\n",
    "lys08: {'acute': 4, 'grave': 0, 'circumflex': 0}\n",
    "ra04: {'acute': 9, 'grave': 0, 'circumflex': 0}\n",
    "ra08: {'acute': 8, 'grave': 0, 'circumflex': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f05bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polystrophic cantica:  ['ach01', 'ach05', 'eq07', 'pax01', 'lys08', 'ra04', 'ra08']\n",
      "Statistics for ach01:\n",
      "  Acute: 18 / 62 = 0.29\n",
      "  Grave: 4 / 27 = 0.15\n",
      "  Circumflex: 8 / 32 = 0.25\n",
      "\n",
      "Statistics for ach05:\n",
      "  Acute: 0 / 28 = 0.00\n",
      "  Grave: 0 / 12 = 0.00\n",
      "  Circumflex: 0 / 16 = 0.00\n",
      "\n",
      "Statistics for eq07:\n",
      "  Acute: 0 / 76 = 0.00\n",
      "  Grave: 0 / 18 = 0.00\n",
      "  Circumflex: 0 / 26 = 0.00\n",
      "\n",
      "Statistics for pax01:\n",
      "  Acute: 6 / 79 = 0.08\n",
      "  Grave: 3 / 39 = 0.08\n",
      "  Circumflex: 0 / 23 = 0.00\n",
      "\n",
      "Statistics for lys08:\n",
      "  Acute: 4 / 104 = 0.04\n",
      "  Grave: 0 / 57 = 0.00\n",
      "  Circumflex: 0 / 51 = 0.00\n",
      "\n",
      "Statistics for ra04:\n",
      "  Acute: 9 / 28 = 0.32\n",
      "  Grave: 0 / 18 = 0.00\n",
      "  Circumflex: 0 / 5 = 0.00\n",
      "\n",
      "Statistics for ra08:\n",
      "  Acute: 8 / 47 = 0.17\n",
      "  Grave: 0 / 11 = 0.00\n",
      "  Circumflex: 0 / 10 = 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "from src.stats import accentually_responding_syllables_of_strophes_polystrophic, count_all_accents_canticum\n",
    "from src.utils.utils import polystrophic_cantica\n",
    "\n",
    "polystrophic_cantica.insert(0, \"ach01\")\n",
    "\n",
    "for canticum in polystrophic_cantica:\n",
    "    infix = canticum[:-2]\n",
    "    xml_file = f\"data/compiled/responsion_{infix}_compiled.xml\"\n",
    "    tree = etree.parse(xml_file)\n",
    "\n",
    "    strophes = tree.xpath(f'//*[self::strophe or self::antistrophe][@responsion=\"{canticum}\"]')\n",
    "    accent_maps = accentually_responding_syllables_of_strophes_polystrophic(*strophes)\n",
    "\n",
    "    total_accent_sums = count_all_accents_canticum(tree, canticum)\n",
    "    accent_responsion_counts = {\n",
    "        'acute': sum(len(d) for d in accent_maps[0]),\n",
    "        'grave': sum(len(d) for d in accent_maps[1]),\n",
    "        'circumflex': sum(len(d) for d in accent_maps[2])\n",
    "    }\n",
    "\n",
    "    acute_stat = accent_responsion_counts['acute'] / total_accent_sums['acute'] if total_accent_sums['acute'] > 0 else 0\n",
    "    grave_stat = accent_responsion_counts['grave'] / total_accent_sums['grave'] if total_accent_sums['grave'] > 0 else 0\n",
    "    circumflex_stat = accent_responsion_counts['circumflex'] / total_accent_sums['circumflex'] if total_accent_sums['circumflex'] > 0 else 0\n",
    "\n",
    "    print(f\"Statistics for {canticum}:\")\n",
    "    print(f\"  Acute: {accent_responsion_counts['acute']} / {total_accent_sums['acute']} = {acute_stat:.2f}\")\n",
    "    print(f\"  Grave: {accent_responsion_counts['grave']} / {total_accent_sums['grave']} = {grave_stat:.2f}\")\n",
    "    print(f\"  Circumflex: {accent_responsion_counts['circumflex']} / {total_accent_sums['circumflex']} = {circumflex_stat:.2f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bffb046",
   "metadata": {},
   "source": [
    "## Barys Responsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7b3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
